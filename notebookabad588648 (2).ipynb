{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11281984,"sourceType":"datasetVersion","datasetId":7053626},{"sourceId":11291999,"sourceType":"datasetVersion","datasetId":7060592},{"sourceId":11294185,"sourceType":"datasetVersion","datasetId":7062026},{"sourceId":11294302,"sourceType":"datasetVersion","datasetId":7062116},{"sourceId":11294720,"sourceType":"datasetVersion","datasetId":7062410},{"sourceId":11296081,"sourceType":"datasetVersion","datasetId":7063483}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install lightning\n!pip install fastexcel","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T10:01:50.110153Z","iopub.execute_input":"2025-04-07T10:01:50.110539Z","iopub.status.idle":"2025-04-07T10:02:02.192197Z","shell.execute_reply.started":"2025-04-07T10:01:50.110508Z","shell.execute_reply":"2025-04-07T10:02:02.190765Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport polars as pl\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\ndf = pl.read_excel('/kaggle/input/jk-pinn-2404-3/().xlsx')\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T10:02:02.193504Z","iopub.execute_input":"2025-04-07T10:02:02.193838Z","iopub.status.idle":"2025-04-07T10:02:04.017579Z","shell.execute_reply.started":"2025-04-07T10:02:02.193807Z","shell.execute_reply":"2025-04-07T10:02:04.016554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = df.select(pl.col('应变幅度'), pl.col('保持时间'), pl.col('应变率'), pl.col('温度T'), pl.col('C'), pl.col('Ni'), pl.col('Co'), pl.col('Cr'), pl.col('Mo'), pl.col('Al'), pl.col('Ti')).to_numpy()\ny = df.select(pl.col('循环次数')).to_numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T10:02:04.050704Z","iopub.execute_input":"2025-04-07T10:02:04.051102Z","iopub.status.idle":"2025-04-07T10:02:04.076019Z","shell.execute_reply.started":"2025-04-07T10:02:04.051073Z","shell.execute_reply":"2025-04-07T10:02:04.074827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#from tsaug import TimeWarp, Crop, Quantize, Drift, Reverse, AddNoise\n#augmenter = (\n#    AddNoise(scale=0.03)\n#)\n#\n#Xy = np.hstack([X, y])\n#Xy = np.vstack([Xy ,augmenter.augment(Xy)])\n#Xy = np.vstack([Xy ,augmenter.augment(Xy)])\n#Xy = np.vstack([Xy ,augmenter.augment(Xy)])\n#Xy = np.vstack([Xy ,augmenter.augment(Xy[:32, :])])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T10:02:04.077100Z","iopub.execute_input":"2025-04-07T10:02:04.077439Z","iopub.status.idle":"2025-04-07T10:02:04.081439Z","shell.execute_reply.started":"2025-04-07T10:02:04.077411Z","shell.execute_reply":"2025-04-07T10:02:04.080423Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nfrom torch import optim, nn, utils, Tensor\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\nimport lightning as L\n\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\nfrom torch.utils.data import Dataset, DataLoader\nfrom lightning.pytorch.callbacks.early_stopping import EarlyStopping\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n# define any number of nn.Modules (or use your current ones)\n\n\n# define the LightningModule\nclass LitMlpModel(L.LightningModule):\n    def __init__(self, lamb):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(11, 36), nn.GELU(), nn.BatchNorm1d(36, 36), nn.Dropout(0.1))\n        self.fc2 = nn.Sequential(nn.Linear(36, 36), nn.GELU(), nn.BatchNorm1d(36, 36), nn.Dropout(0.1))\n        self.fc3 = nn.Sequential(nn.Linear(36, 1))\n        self.relu = nn.ReLU()\n        self.train_losses = []\n        self.valid_losses = []\n        self.train_losses_epoch = []\n        self.valid_losses_epoch = []\n        self.phy_losses = []\n        self.phy_losses_epoch = []\n        self.partial_deri_1 = []\n        self.partial_deri_2 = []\n        self.partial_deri_3 = []\n        self.partial_deri_1_final = []\n        self.partial_deri_2_final = []\n        self.partial_deri_3_final = []\n        self.lamb = lamb\n        \n    def forward(self, batch):\n        x, y= batch\n        return self.fc3(self.fc2(self.fc2(self.fc2(self.fc1(x)))))\n        \n    def training_step(self, batch, batch_idx):\n        # training_step defines the train loop.\n        # it is independent of forward\n        x, y = batch\n        #y = torch.log10(y)\n        y_hat = self.fc3(self.fc2(self.fc2(self.fc2(self.fc2(self.fc1(x))))))\n\n        y_hat_x1 = torch.autograd.grad(y_hat, x, grad_outputs=torch.ones_like(y_hat), create_graph=True)[0][:, [0]]\n        y_hat_x2 = torch.autograd.grad(y_hat, x, grad_outputs=torch.ones_like(y_hat), create_graph=True)[0][:, [1]]\n        y_hat_x2x2 = -torch.autograd.grad(y_hat_x2, x, grad_outputs=torch.ones_like(y_hat), create_graph=True)[0][:, [1]]\n        \n        #torch.sum(y_hat_x1) + torch.sum(y_hat_x2) + torch.sum(y_hat_x2x2)\n        lamb = self.lamb\n        phy_loss = (torch.sum(self.relu(y_hat_x1)) + torch.sum(self.relu(y_hat_x2)) + torch.sum(self.relu(y_hat_x2x2))) / len(y_hat_x1)\n        loss = (1 - lamb) * nn.functional.mse_loss(y_hat, y) + lamb * (phy_loss)\n        # Logging to TensorBoard (if installed) by default\n        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True)\n        self.train_losses.append(loss.item())  # 保存 loss 数值\n        self.phy_losses.append(phy_loss.item())\n        self.partial_deri_1 += y_hat_x1\n        self.partial_deri_2 += y_hat_x2\n        self.partial_deri_3 += y_hat_x2x2\n        \n        return loss\n\n    def on_train_epoch_end(self):\n        train_loss_epoch = torch.mean(torch.tensor(self.train_losses, dtype=torch.float32))\n        self.train_losses_epoch.append(train_loss_epoch)\n        self.train_losses.clear()  # free memory\n\n        phy_loss_epoch = torch.mean(torch.tensor(self.phy_losses, dtype=torch.float32))\n        self.phy_losses_epoch.append(phy_loss_epoch)\n        self.phy_losses.clear()  # free memory\n        \n        self.partial_deri_1_final = self.partial_deri_1.copy()\n        self.partial_deri_1.clear()\n        self.partial_deri_2_final = self.partial_deri_2.copy()\n        self.partial_deri_2.clear()\n        self.partial_deri_3_final = self.partial_deri_3.copy()\n        self.partial_deri_3.clear()\n\n    def validation_step(self, batch, batch_idx):\n        # training_step defines the train loop.\n        # it is independent of forward\n        x, y = batch\n        #y = torch.log10(y)\n        y_hat = self.fc3(self.fc2(self.fc2(self.fc2(self.fc1(x)))))\n\n        #y_hat_x1 = self.relu(torch.autograd.grad(y_hat, x, grad_outputs=torch.ones_like(y_hat), create_graph=True)[0][:, [0]])\n        #y_hat_x2 = self.relu(torch.autograd.grad(y_hat, x, grad_outputs=torch.ones_like(y_hat), create_graph=True)[0][:, [1]])\n        #y_hat_x2x2 = self.relu(-torch.autograd.grad(y_hat_x2, x, grad_outputs=torch.ones_like(y_hat), create_graph=True)[0][:, [1]])\n        \n        loss = nn.functional.mse_loss(y_hat, y) #+ (torch.sum(y_hat_x1) + torch.sum(y_hat_x2) + torch.sum(y_hat_x2x2)) / len(y_hat_x1)\n        # Logging to TensorBoard (if installed) by default\n        self.log(\"valid_loss\", loss, on_epoch=True, prog_bar=True)\n        self.valid_losses.append(loss.item())  # 保存 loss 数值\n        return loss\n\n    def on_validation_epoch_end(self):\n        valid_loss_epoch = torch.mean(torch.tensor(self.valid_losses, dtype=torch.float32))\n        self.valid_losses_epoch.append(valid_loss_epoch)\n        self.valid_losses.clear()  # free memory\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n        return optimizer\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T10:02:04.082479Z","iopub.execute_input":"2025-04-07T10:02:04.082958Z","iopub.status.idle":"2025-04-07T10:02:17.035802Z","shell.execute_reply.started":"2025-04-07T10:02:04.082927Z","shell.execute_reply":"2025-04-07T10:02:17.034729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: 定义自定义 Dataset\nclass MyTabularDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X  # 特征\n        self.y = y   # 标签\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        # 返回张量格式\n        X = torch.tensor(self.X[idx], dtype=torch.float32, requires_grad=True)\n        y = torch.tensor(self.y[idx], dtype=torch.float32, requires_grad=True)  # 或 long 对于分类\n        return X, y\n\n# Step 2: 创建 Dataset 实例\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=6, shuffle=True, random_state=42)\n\n\n\nphy_losses_lamb = {}\ny_pred_lamb = {}\npartial_lamb = {}\n\nfor lamb in [0.0, 0.6]:\n    y_pred = np.zeros_like(y)\n    for i, (trn_idx, val_idx) in enumerate(kf.split(X)):\n        scaler = StandardScaler().fit(X[trn_idx, :])\n        X_std_trn = scaler.transform(X[trn_idx, :])\n        X_std_val = scaler.transform(X[val_idx, :])\n        \n        train_loader = DataLoader(MyTabularDataset(X_std_trn, y[trn_idx]), batch_size=8, shuffle=True)\n        valid_loader = DataLoader(MyTabularDataset(X_std_val, y[val_idx]), batch_size=8, shuffle=True)\n        \n        early_stop_callback = EarlyStopping(\n            monitor=\"valid_loss\",       # 监控的指标，通常是验证损失\n            patience=50,               # 如果连续 3 次没有提升就停止\n            mode=\"min\",               # 因为是 loss，越小越好\n            verbose=False              # 是否打印日志\n        )\n        # init the autoencoder\n        \n        mlpmodel = LitMlpModel(lamb)\n        # train the model (hint: here are some helpful Trainer arguments for rapid idea iteration)\n        trainer = L.Trainer(limit_train_batches=100000, max_epochs=500, callbacks=[early_stop_callback])\n        trainer.fit(mlpmodel, train_loader, valid_loader)\n        \n        y_pred[val_idx] = mlpmodel((torch.tensor(X_std_val, dtype=torch.float32, requires_grad=True), torch.tensor(y[val_idx], dtype=torch.float32, requires_grad=True))).detach().numpy()\n        \n        plt.figure(figsize=(12, 8), dpi=100)\n        plt.plot(mlpmodel.train_losses_epoch, label='Train_Losses')\n        plt.plot(mlpmodel.valid_losses_epoch, label='Valid_Losses')\n        plt.title(f\"Training Loss Curve Fold {i+1} with Lambda {round(lamb,1)}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.savefig(f\"Loss_Curve_{i+1}_with_Lambda_{lamb}.png\")\n        plt.show()\n        phy_losses_lamb[f'Fold_{i+1}_Lamb_{lamb}'] = mlpmodel.phy_losses_epoch\n\n    partial_lamb[f'Lamb_{lamb}_1'] = mlpmodel.partial_deri_1_final\n    partial_lamb[f'Lamb_{lamb}_2'] = mlpmodel.partial_deri_2_final\n    partial_lamb[f'Lamb_{lamb}_3'] = mlpmodel.partial_deri_3_final\n    y_pred_lamb[f'Lamb_{lamb}'] = y_pred\n    \n    #print(f'Lambda {lamb} MAPE Ncf: ', mean_absolute_percentage_error(y, 10**y_pred))\n    #print(f'Lambda {lamb} MSE Ncf: ', mean_squared_error(y, 10**y_pred))\n    #print(f'Lambda {lamb} MAPE lgNcf: ', mean_absolute_percentage_error(np.log10(y), y_pred))\n    #print(f'Lambda {lamb} MSE lgNcf: ', mean_squared_error(np.log10(y), y_pred))\n    \n    print(f'Lambda {lamb} MAPE Ncf: ', mean_absolute_percentage_error(y, y_pred))\n    print(f'Lambda {lamb} MSE Ncf: ', mean_squared_error(y, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T10:02:17.036819Z","iopub.execute_input":"2025-04-07T10:02:17.037366Z","iopub.status.idle":"2025-04-07T10:02:58.297289Z","shell.execute_reply.started":"2025-04-07T10:02:17.037332Z","shell.execute_reply":"2025-04-07T10:02:58.296144Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.tensor(partial_lamb[f'Lamb_{0.0}_{1}'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T10:02:58.298318Z","iopub.execute_input":"2025-04-07T10:02:58.298588Z","iopub.status.idle":"2025-04-07T10:02:58.316936Z","shell.execute_reply.started":"2025-04-07T10:02:58.298564Z","shell.execute_reply":"2025-04-07T10:02:58.315686Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x1_dnn = torch.tensor(partial_lamb[f'Lamb_{0.0}_{1}']).detach().numpy()\nx1_pinn = torch.tensor(partial_lamb[f'Lamb_{0.6}_{1}']).detach().numpy()\n\nx2_dnn = torch.tensor(partial_lamb[f'Lamb_{0.0}_{2}']).detach().numpy()\nx2_pinn = torch.tensor(partial_lamb[f'Lamb_{0.6}_{2}']).detach().numpy()\n\nx3_dnn = torch.tensor(partial_lamb[f'Lamb_{0.0}_{3}']).detach().numpy()\nx3_pinn = torch.tensor(partial_lamb[f'Lamb_{0.6}_{3}']).detach().numpy()\n\n# 创建索引用于 y 轴\ny_ = np.arange(1, len(x1_dnn) + 1)\n\n# 子图设置\nfig, axs = plt.subplots(1, 3, figsize=(16, 5), sharey=True)\n\n# 子图标题与标签\ntitles = ['(a)', '(b)', '(c)']\nxlabels = [r'$\\partial N_{cf}/\\partial \\Delta \\varepsilon_a$', \n           r'$\\partial N_{cf}/\\partial t_h$', \n           r'$\\partial^2 N_{cf}/\\partial t_h^2$']\nxvals = [(x1_dnn, x1_pinn), (x2_dnn, x2_pinn), (x3_dnn, x3_pinn)]\n\nfor i in range(3):\n    ax = axs[i]\n    dnn, pinn = xvals[i]\n\n    # 连线\n    for j in range(len(x1_dnn)):\n        ax.plot([dnn[j], pinn[j]], [y_[j], y_[j]], color='gray', linewidth=0.8)\n\n    # 散点\n    ax.scatter(dnn, y_, label='DNN')\n    ax.scatter(pinn, y_, edgecolor='black', label='PINN')\n\n    # 阴影区域\n    #ax.axvspan(shaded_regions[i][0], shaded_regions[i][1], ymin=0, ymax=1, color='red', alpha=0.15)\n\n    # 文本注释\n    #ax.text(np.mean(shaded_regions[i]), 20, 'Inconsistent\\nwith Physics', color='brown', fontsize=10)\n\n    # 标签与标题\n    ax.set_title(titles[i], loc='left', fontsize=12)\n    ax.set_xlabel(xlabels[i], fontsize=12)\n    #ax.set_xlim(xlims[i])\n    ax.grid(True, linestyle='--', alpha=0.5)\n\n# Y轴标签统一放在左边\naxs[0].set_ylabel('Creep-fatigue data points', fontsize=12)\n\n# 图例只加一次\naxs[0].legend()\n\nplt.tight_layout()\nplt.savefig('Creep-fatigue data points.png')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T10:02:58.318129Z","iopub.execute_input":"2025-04-07T10:02:58.318471Z","iopub.status.idle":"2025-04-07T10:02:59.868942Z","shell.execute_reply.started":"2025-04-07T10:02:58.318441Z","shell.execute_reply":"2025-04-07T10:02:59.867690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#for i in range(6):\n#    plt.figure(figsize=(12, 8), dpi=100)\n#    for lamb in np.arange(0, 1, 0.2):\n#        plt.plot(phy_losses_lamb[f'Fold_{i+1}_Lamb_{lamb}'], label=f'Lambda={round(lamb, 1)}')\n#    plt.legend()\n#    plt.ylabel('Physics Loss')\n#    plt.xlabel('Epoch')\n#    plt.title(f'Physics Loss Fold {i+1} with Lambda {round(lamb, 1)}')\n#    plt.savefig(f'Physics Loss Fold {i+1} with Lambda {int(lamb*10)}.png')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T10:02:59.870325Z","iopub.execute_input":"2025-04-07T10:02:59.870791Z","iopub.status.idle":"2025-04-07T10:02:59.875841Z","shell.execute_reply.started":"2025-04-07T10:02:59.870747Z","shell.execute_reply":"2025-04-07T10:02:59.874352Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"times = df.select(pl.col('循环次数')).to_numpy().ravel()\nnp.random.seed(42)\n\nmarkers = [\"o\", \"v\", \"^\", \"<\", \">\", \"p\"]\n\nplt.figure(figsize=(12, 8), dpi=200)\nplt.grid(True)  # 显示网格线\n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=6, shuffle=True, random_state=42)\n\nfor i, ((trn_idx, val_idx), mark) in enumerate(zip(kf.split(times), markers)):\n    plt.scatter(y[val_idx], y_pred_lamb[f'Lamb_{0.0}'][val_idx], label=f'Fold {1+i}', marker=mark)\n    \n\nplt.plot([y.min()*0.5, y.max()*2], [y.min()*0.5, y.max()*2], color='blue', alpha=0.5)\nfactor = 0.25\nplt.fill_between([y.min()*0.5, times.max()*2], [y.min()*0.5*(factor), y.max()*2*factor], [y.min()*0.5/factor, y.max()*2/factor], alpha=0.1, label='±2σ')\n\nplt.legend()\nplt.xscale('log')     # 对横轴取 log\nplt.yscale('log')\nplt.xlim((y.min()-10, y.max()+3000))\nplt.ylim((y.min()-10, y.max()+3000))\nplt.xlabel('Experimental creep-fatigue life')\nplt.ylabel('Predicted creep-fatigue life')\nplt.title(f'Inconel-617 Creep-Fatigue Life Prediction with PINN')\nplt.savefig(f'Inconel-617 Creep-Fatigue Life Prediction with PINN.png')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T10:05:19.146172Z","iopub.execute_input":"2025-04-07T10:05:19.146587Z","iopub.status.idle":"2025-04-07T10:05:19.496660Z","shell.execute_reply.started":"2025-04-07T10:05:19.146555Z","shell.execute_reply":"2025-04-07T10:05:19.495100Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"times = df.select(pl.col('循环次数')).to_numpy().ravel()\nnp.random.seed(42)\n\nmarkers = [\"o\", \"v\", \"^\", \"<\", \">\", \"p\"]\n\nplt.figure(figsize=(12, 8), dpi=200)\nplt.grid(True)  # 显示网格线\n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=6, shuffle=True, random_state=42)\n\nfor i, ((trn_idx, val_idx), mark) in enumerate(zip(kf.split(times), markers)):\n    plt.scatter(y[val_idx], y_pred_lamb[f'Lamb_{0.6}'][val_idx], label=f'Fold {1+i}', marker=mark)\n    \n\nplt.plot([y.min()*0.5, y.max()*2], [y.min()*0.5, y.max()*2], color='blue', alpha=0.5)\nfactor = 0.25\nplt.fill_between([y.min()*0.5, times.max()*2], [y.min()*0.5*(factor), y.max()*2*factor], [y.min()*0.5/factor, y.max()*2/factor], alpha=0.1, label='±2σ')\n\nplt.legend()\nplt.xscale('log')     # 对横轴取 log\nplt.yscale('log')\nplt.xlim((y.min()-10, y.max()+3000))\nplt.ylim((y.min()-10, y.max()+3000))\nplt.xlabel('Experimental creep-fatigue life')\nplt.ylabel('Predicted creep-fatigue life')\nplt.title(f'Inconel-617 Creep-Fatigue Life Prediction with DNN')\nplt.savefig(f'Inconel-617 Creep-Fatigue Life Prediction with DNN.png')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T10:03:00.825435Z","iopub.status.idle":"2025-04-07T10:03:00.825783Z","shell.execute_reply":"2025-04-07T10:03:00.825652Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}